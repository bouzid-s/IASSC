{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <a id='toc1_'></a><div style=\"text-align:center; border-radius:15px 50px; padding:15px; color:white; margin:0; font-size:150%; font-family:Pacifico; background-color:#2a6199; overflow:hidden\"><b> üß†  TP1 - Quantification des mod√®les IA </b>\n",
    "\n",
    "Ce notebook d√©montre comment appliquer des techniques de **quantification** aux mod√®les IA pour am√©liorer leurs performances et r√©duire leur taille. Vous apprendrez :\n",
    "- Les concepts cl√©s de la **quantification des mod√®les**.\n",
    "- L'application de la quantification sur un exemple simple.\n",
    "- La quantification d'un mod√®le d'image plus complexe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ---\n",
    "  ### <a id='toc1_1_1_'></a>[**Table des mati√®res**](#toc0_)\n",
    "\n",
    "- [üìò 1. Introduction √† la quantification](#toc1_)    \n",
    "  - [1.1 Types de quantification](#toc3_)    \n",
    "  - [1.2 Les compromis √† prendre en compte](#toc4_)    \n",
    "- [üîç 2. √âchauffement : Bases de la quantification](#toc5_)    \n",
    "  - [2.1 Cr√©ation d'une s√©rie de valeurs](#toc6_)    \n",
    "  - [2.2 Quantification Asym√©trique](#toc7_)    \n",
    "  - [2.3 Quantification Sym√©trique](#toc8_)    \n",
    "  - [2.4 M√©thode de quantification par Percentile](#toc9_)    \n",
    "  - [2.5 Comparaison : Symetrique, Asymetrique, Min-Max , Percentile](#toc10_)    \n",
    "  - [2.6 Analyse de l'impact du nombre des bits sur l'erreur](#toc11_)    \n",
    "- [üñºÔ∏è 3. Application aux Mod√®les Complexes](#toc12_)    \n",
    "  - [3.1 Quantification apr√©s entrainement (Post Training Quantization : PTQ)](#toc13_)    \n",
    "  - [3.2 Quantification pendant l'entra√Ænement (Quantization Aware Training : QAT)](#toc22_)    \n",
    " \n",
    "- [üìä 4. √âvaluation et comparaison](#toc25_)    \n",
    "  - [4.1 Comparaison des tailles et des performances des mod√®les](#toc26_)    \n",
    "  - [4.2 Conclusion](#toc29_)      \n",
    "- [üéÅ Bonus](#toc32_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n",
    "  ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[<div style=\"text-align:center; border-radius:8px; padding:8px; color:white; margin:10px 0; font-size:100%; font-family:Arial, sans-serif; background-color:#1E90FF;\"><b>üìò 1. Introduction √† la quantification</b></div>](#toc2_)\n",
    "# \n",
    "\n",
    "En traitement du signal num√©rique, la quantification fait g√©n√©ralement r√©f√©rence √† la conversion d‚Äôun signal continu, comme la lumi√®re ou le son, en un ensemble de valeurs num√©riques discr√®tes. Dans le domaine de l‚Äôapprentissage profond, la quantification d√©signe g√©n√©ralement la conversion de nombres en virgule flottante sign√©e en pr√©cision simple (float32) en un format num√©rique de pr√©cision inf√©rieure, tel qu‚Äôun entier non sign√© sur 8 bits (uint8). Elle permet :\n",
    "\n",
    "- Une **r√©duction de la taille des mod√®les**.\n",
    "- Une **acc√©l√©ration des inf√©rences**.\n",
    "- Un **support pour le mat√©riel contraint** (par exemple : IoT, t√©l√©phones mobiles).\n",
    "\n",
    "# <a id='toc3_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b>1.1 Types de quantification</b></div>](#toc3_)\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/bouzid-s/IASSC/blob/main/sources/types_quant.png?raw=true\" alt=\"typesQuant\" width=\"700\">\n",
    "\n",
    "# <a id='toc4_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b>1.2 Les compromis √† prendre en compte</b></div>](#toc4_)\n",
    "\n",
    "\n",
    "\n",
    "1. **Taille du mod√®le :**\n",
    "   - R√©duction typique de 75% pour des poids convertis de Float32 √† INT8.\n",
    "\n",
    "2. **Pr√©cision :**\n",
    "   - La quantification peut entra√Æner une perte de pr√©cision due √† l‚Äôarrondi des valeurs.\n",
    "   - **Sym√©trique :** Plus adapt√©e pour les poids distribu√©s autour de z√©ro.\n",
    "   - **Asym√©trique :** Utile pour pr√©server la pr√©cision des activations positives.\n",
    "\n",
    "3. **Vitesse :**\n",
    "   - Les mod√®les quantifi√©s sont plus rapides en inf√©rence, surtout sur des processeurs sp√©cialis√©s (comme les DSP ou TPU).\n",
    "\n",
    "4. **Compatibilit√© mat√©rielle :**\n",
    "   - Certains mat√©riels favorisent la quantification asym√©trique pour les activations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[<div style=\"text-align:center; border-radius:8px; padding:8px; color:white; margin:10px 0; font-size:100%; font-family:Arial, sans-serif; background-color:#1E90FF;\"><b>üîç 2. √âchauffement : Bases de la quantification</b></div>](#toc5_)\n",
    "# \n",
    "\n",
    "  ---\n",
    "  ### <a id='toc1_1_1_'></a>[**Table des mati√®res**](#toc0_)\n",
    "\n",
    "- [1. Introduction √† la quantification](#toc1_)    \n",
    "  - [1.1 Types de quantification](#toc3_)    \n",
    "  - [1.2 Les compromis √† prendre en compte](#toc4_)    \n",
    "- [üîç 2. √âchauffement : Bases de la quantification](#toc5_)    \n",
    "  - [2.1 Cr√©ation d'une s√©rie de valeurs](#toc6_)    \n",
    "  - [2.2 Quantification Asym√©trique](#toc7_)    \n",
    "  - [2.3 Quantification Sym√©trique](#toc8_)    \n",
    "  - [2.4 M√©thode de quantification par Percentile](#toc9_)    \n",
    "  - [2.5 Comparaison : Symetrique, Asymetrique, Min-Max , Percentile](#toc10_)    \n",
    "  - [2.6 Analyse de l'impact du nombre des bits sur l'erreur](#toc11_)    \n",
    "- [üñºÔ∏è 3. Application aux Mod√®les Complexes](#toc12_)    \n",
    "  - [3.1 Quantification apr√©s entrainement (Post Training Quantization PTQ)](#toc13_)    \n",
    "  - [3.2 Quantification pendant l'entra√Ænement (Quantization Aware Training)](#toc22_)    \n",
    " \n",
    "- [üìä 4. √âvaluation et comparaison](#toc25_)    \n",
    "  - [4.1 Comparaison des tailles et des performances des mod√®les](#toc26_)    \n",
    "  - [4.2 Conclusion](#toc29_)      \n",
    "- [Bonus](#toc32_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "   numbering=false\n",
    "   anchor=true\n",
    "   flat=false\n",
    "   minLevel=1\n",
    "   maxLevel=6\n",
    "   /vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n",
    "---\n",
    "\n",
    "\n",
    "D√©velopper un programme en Python qui g√©n√®re al√©atoirement une s√©rie de valeurs synth√©tiques et applique des m√©thodes de quantification **sym√©trique** et **asym√©trique**. Ce programme devra analyser les impacts de la quantification sur les donn√©es et √©valuer les erreurs associ√©es.\n",
    "\n",
    "---\n",
    "### √âtapes √† suivre :\n",
    "\n",
    "1. **Cr√©ation d'une s√©rie de valeurs :**\n",
    "   - G√©n√©rer un vecteur al√©atoire ou utiliser un ensemble fixe de valeurs.\n",
    "\n",
    "2. **Quantification :**\n",
    "   - Impl√©menter les m√©thodes de quantification sym√©trique et asym√©trique.\n",
    "   - Tester pour diff√©rents intervalles $[\\alpha, \\beta]$.\n",
    "\n",
    "1. **D√©quantification :**\n",
    "   - Revenir aux valeurs approximatives dans l'√©chelle initiale.\n",
    "\n",
    "2. **Calcul de l'erreur de quantification :**\n",
    "   - Calculer l'erreur quadratique moyenne (MSE) pour chaque m√©thode de quantification.\n",
    "\n",
    "---\n",
    "\n",
    "### R√©sultats attendus :\n",
    "- Afficher les valeurs originales, quantifi√©es, et d√©quantifi√©es.\n",
    "- Visualiser les erreurs de quantification sous forme graphique.\n",
    "- Calculer et comparer les erreurs entre les deux m√©thodes.\n",
    "- L'erreur sera calcul√©e en utilisant la formule suivante :\n",
    "$  MSE = \\frac{1}{n} \\sum_{i=1}^n \\left( x_{\\text{original}, i} - x_{\\text{d√©quantifi√©}, i} \\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow\", tf.__version__)\n",
    "print(\"Python\", sys.version)\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "def calculate_errors(original, dequantized):\n",
    "    mse = np.mean((original - dequantized)**2)\n",
    "    mae = np.mean(np.abs(original - dequantized))\n",
    "    return mse, mae\n",
    "\n",
    "def plot_original_vs_quantized(x_original, x_quant, title=\"Original vs Quantifi√© \", c='gray'):\n",
    "    # Cr√©ation des sous-graphiques\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Premier sous-graphe : Original vs Quantifi√©\n",
    "    plt.scatter(x_original, x_quant, color='orange', alpha=0.7, label=\"Quantifi√© \")\n",
    "    plt.plot(x_original, x_original, color='red', linestyle='--', label=\"Ligne parfaite (y=x)\")\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Valeurs Quantifi√©es\", fontsize=12)\n",
    "    plt.grid(linestyle='--', alpha=0.6)\n",
    "    plt.legend(fontsize=10, frameon=True, loc='upper left')\n",
    "    # Ajustement des espaces entre les sous-graphiques\n",
    "    plt.tight_layout()\n",
    "    # Affichage du graphique\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_histograms(x_original, x_dequant_asym, title=\"Original vs DeQuantifi√© \", c='gray'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(x_original, bins=50, alpha=0.7, label=\"Original\",color='orange', edgecolor='black', linewidth=0.5)\n",
    "    plt.hist(x_dequant_asym, bins=50, alpha=0.5, label=\"D√©quantifi√©\",  color=c, edgecolor='black', linewidth=0.5)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Fr√©quence\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.legend(frameon=True, fontsize=10, loc='upper right')\n",
    "\n",
    "def plot_error(x_original, x_dequant, title, c='gray'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Calcul des erreurs absolues pour chaque type de quantification\n",
    "    error = np.abs(x_original - x_dequant)\n",
    "\n",
    "    # Calcul des moyennes d'erreur\n",
    "    mean_error = np.mean(error)\n",
    "        # Lignes horizontales pour les moyennes\n",
    "    # Premier sous-graphe : Erreur pour la quantification asym√©trique\n",
    "    plt.plot(error, label=\"Erreur (\"+title+\")\", color=c, linestyle='-', marker='o', alpha=0.7)\n",
    "    plt.title(\"Erreur de Quantification : \"+title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Erreur absolue\", fontsize=12)\n",
    "    plt.axhline(mean_error, color='red', linestyle='--', label=f\"Moyenne : {mean_error:.2f}\")\n",
    "    plt.legend(fontsize=10, frameon=True, loc='upper right')\n",
    "    plt.grid(linestyle='--', alpha=0.6)\n",
    "    # Ajout des labels et du titre\n",
    "    plt.xlabel(\"Index des valeurs\", fontsize=12)\n",
    "    # Ajustement des espaces entre les sous-graphiques\n",
    "    plt.tight_layout()\n",
    "    # Affichage du graphique\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 2.1 Cr√©ation d'une s√©rie de valeurs </b></div>](#toc6_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc7_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 2.2 Quantification Asym√©trique </b></div>](#toc7_)\n",
    "\n",
    "##### Quantification asym√©trique :\n",
    "- Convertit une s√©rie de nombres en virgule flottante d‚Äôun intervalle $[\\beta, \\alpha]$ vers l‚Äôintervalle $[0, 2^n - 1]$ (par exemple, avec 8 bits : intervalle $[0, 255]$).\n",
    "\n",
    "**Formules associ√©es :**\n",
    "- Quantification :\n",
    "  \n",
    "  $  x_q = \\text{clamp}\\left(\\left\\lfloor \\frac{x_f}{s} \\right\\rfloor + z; 0, 2^n - 1\\right), \\quad s = \\frac{\\alpha - \\beta}{2^n - 1}, \\quad z = \\left\\lfloor -1 \\times \\frac{\\beta}{s} \\right\\rfloor $\n",
    "\n",
    "- D√©quantification :  \n",
    "\n",
    "  $  x_f = s \\left( x_q - z \\right)$\n",
    "  \n",
    "\n",
    "**Illustration :**\n",
    "\n",
    "<img src=\"https://github.com/bouzid-s/IASSC/blob/main/sources/asym.png?raw=true\" alt=\"Quantification asym√©trique\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc8_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 2.3 Quantification Sym√©trique </b></div>](#toc8_)\n",
    "\n",
    "\n",
    "\n",
    "##### Quantification sym√©trique :\n",
    "- Convertit une s√©rie de nombres en virgule flottante dans l‚Äôintervalle $[-\\alpha, \\alpha]$ vers un intervalle sym√©trique en entiers $[-(2^{n-1} - 1), 2^{n-1} - 1]$. \n",
    "- Par exemple, avec 8 bits, cet intervalle est $[-127, 127]$.\n",
    "\n",
    "**Formules associ√©es :**\n",
    "- Quantification :  \n",
    "- \n",
    "  $  x_q = \\text{clamp}\\left(\\left\\lfloor \\frac{x_f}{s} \\right\\rfloor; -(2^{n-1} - 1), 2^{n-1} - 1\\right), \\quad s = \\frac{\\text{abs}(\\alpha)}{2^{n-1} - 1}$\n",
    "\n",
    "- D√©quantification :  \n",
    "\n",
    "  $  x_f = s \\left( x_q \\right)$\n",
    "\n",
    "**Illustration :**\n",
    "\n",
    "<img src=\"https://github.com/bouzid-s/IASSC/blob/main/sources/sym.png?raw=true\" alt=\"Quantification sym√©trique\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 2.4 M√©thode de quantification par Percentile</b></div>](#toc9_)\n",
    "\n",
    "\n",
    "**Principe :**\n",
    "La m√©thode **Percentile** utilise des percentiles ($P_1$ et $P_{99}$) pour d√©finir l'intervalle de quantification, en ignorant les valeurs aberrantes dans les queues de la distribution.\n",
    "\n",
    "**√âtapes d'impl√©mentation :**\n",
    "1. **Calcul des percentiles :**\n",
    "   - $P_1 = \\text{np.percentile(x, 1)}$\n",
    "   - $P_{99} = \\text{np.percentile(x, 99)}$\n",
    "\n",
    "\n",
    "**Consigne :** \n",
    "Comparez les performances de deux m√©thodes de quantification, **Min-Max** et **Percentile**, en termes de pr√©cision et de robustesse.\n",
    "\n",
    "**Objectif :** \n",
    "\n",
    "Identifier dans quels contextes chaque m√©thode est la plus adapt√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc10_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 2.5 Comparaison : Symetrique, Asymetrique, Min-Max , Percentile </b></div>](#toc10_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc11_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 2.6 Analyse de l'impact du nombre des bits sur l'erreur </b></div>](#toc11_)\n",
    "\n",
    "L'erreur d√©pend de l'intervale des valeurs, de leur distribution et du nombre de bits utilis√©s pour la quantification. Comme les distributions des poids et des entr√©es d'un r√©seau de neurones ne sont pas contr√¥l√©es, nous analysons ici l'impact de la largeur des bits sur l'erreur. \n",
    "\n",
    "#### Consignes :\n",
    "Analysez l'impact de la largeur des bits sur l'erreur absolue moyenne en d√©veloppant un programme qui :\n",
    "\n",
    "- **Quantifie** et **d√©quantifie** les donn√©es pour diff√©rentes largeurs de bits `bitwidths = [i for i in range(2, 16)]\n",
    "`.\n",
    "- **Calcule** l'erreur absolue moyenne pour chaque largeur de bits.\n",
    "- **Visualise** la relation entre la largeur des bits et l'erreur √† l'aide d'un graphique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[<div style=\"text-align:left; border-radius:8px; padding:8px; color:white; margin:10px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#1E90FF;\"><b> üñºÔ∏è 3. Application aux Mod√®les Complexes </b></div>](#toc12_)\n",
    "# \n",
    "\n",
    "Dans cette section, vous appliquerez des techniques de quantification sur des mod√®les complexes, comme ceux utilis√©s pour la classification d'images (par exemple, MNIST). Vous explorerez deux approches principales de quantification : **la quantification apr√®s entra√Ænement (PTQ)**, qui est r√©alis√©e sur un mod√®le d√©j√† entra√Æn√©, et **la quantification durant l'entra√Ænement (QAT)**, qui permet d'incorporer la quantification directement dans le processus d'entra√Ænement. Vous analyserez l'impact de ces deux m√©thodes sur la taille du mod√®le et ses performances.\n",
    "\n",
    "\n",
    "  ---\n",
    "  ### <a id='toc1_1_1_'></a>[**Table des mati√®res**](#toc0_)\n",
    "\n",
    "- [1. Introduction √† la quantification](#toc1_)    \n",
    "  - [1.1 Types de quantification](#toc3_)    \n",
    "  - [1.2 Les compromis √† prendre en compte](#toc4_)    \n",
    "- [üîç 2. √âchauffement : Bases de la quantification](#toc5_)    \n",
    "  - [2.1 Cr√©ation d'une s√©rie de valeurs](#toc6_)    \n",
    "  - [2.2 Quantification Asym√©trique](#toc7_)    \n",
    "  - [2.3 Quantification Sym√©trique](#toc8_)    \n",
    "  - [2.4 M√©thode de quantification par Percentile](#toc9_)    \n",
    "  - [2.5 Comparaison : Symetrique, Asymetrique, Min-Max , Percentile](#toc10_)    \n",
    "  - [2.6 Analyse de l'impact du nombre des bits sur l'erreur](#toc11_)    \n",
    "- [üñºÔ∏è 3. Application aux Mod√®les Complexes](#toc12_)    \n",
    "  - [3.1 Quantification apr√©s entrainement (Post Training Quantization PTQ)](#toc13_)    \n",
    "  - [3.2 Quantification pendant l'entra√Ænement (Quantization Aware Training)](#toc22_)    \n",
    " \n",
    "- [üìä 4. √âvaluation et comparaison](#toc25_)    \n",
    "  - [4.1 Comparaison des tailles et des performances des mod√®les](#toc26_)    \n",
    "  - [4.2 Conclusion](#toc29_)      \n",
    "- [Bonus](#toc32_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "    numbering=false\n",
    "    anchor=true\n",
    "    flat=false\n",
    "    minLevel=1\n",
    "    maxLevel=6\n",
    "    /vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n",
    "  ---\n",
    "\n",
    "# <a id='toc13_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 3.1 Quantification apr√©s entrainement (Post Training Quantization : PTQ) </b></div>](#toc13_)\n",
    "\n",
    "\n",
    "\n",
    "La quantification apr√®s entra√Ænement (PTQ) consiste √† convertir un mod√®le d√©j√† entra√Æn√© en un format plus compact. Nous √©valuerons ici l‚Äôimpact de cette m√©thode sur la taille et la pr√©cision du mod√®le.\n",
    "\n",
    "\n",
    "# <a id='toc14_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> a) D√©finition d‚Äôun mod√®le de classification sur MNIST </b></div>](#toc14_)\n",
    "\n",
    "\n",
    "\n",
    "**Consigne :**  \n",
    "\n",
    "D√©veloppez un programme en Python pour cr√©er et compiler un mod√®le de r√©seau de neurones simple en utilisant TensorFlow/Keras. \n",
    "\n",
    "**Instructions**\n",
    "- Impl√©mentez un mod√®le en utilisant `tf.keras.Sequential`.\n",
    "- Int√©grez les couches suivantes :\n",
    "  - **Flatten** : Cette couche doit recevoir une entr√©e de forme `(28, 28)` et l‚Äôaplatir en un vecteur unidimensionnel.\n",
    "  - **Dense** : Ajoutez une couche dense avec 128 unit√©s et la fonction d‚Äôactivation **ReLU**.\n",
    "  - **Dropout** : Ins√©rez une couche de r√©gularisation avec un taux de dropout de 0.2 pour r√©duire le surapprentissage.\n",
    "  - **Dense** : Terminez par une couche dense avec 10 unit√©s et la fonction d‚Äôactivation **softmax** pour effectuer la classification.\n",
    "- Compilez le mod√®le en sp√©cifiant :\n",
    "  - Optimiseur : `Adam` avec un taux d'apprentissage d√©fini par la constante `LEARNING_RATE`.\n",
    "  - Fonction de perte : `SparseCategoricalCrossentropy`.\n",
    "  - M√©trique : `accuracy`.\n",
    "- Retournez le mod√®le √† l‚Äôaide d‚Äôune fonction `create_model()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#--------------------PARAMs--------------------#\n",
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "SEED = 42\n",
    "MODEL_DIR = \"./my_models/\"\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(ds_train, ds_validation, ds_test), ds_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=['train[:85%]', 'train[85%:95%]', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "# Normalize the dataset\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "ds_train = ds_train.map(normalize_img).batch(BATCH_SIZE).shuffle(10000, seed = SEED)\n",
    "ds_validation = ds_validation.map(normalize_img).batch(BATCH_SIZE)\n",
    "ds_test = ds_test.map(normalize_img).batch(1)  # Batch size set to 1 for testing\n",
    "\n",
    "# Define the model\n",
    "def create_model():\n",
    "    \"\"\"Create and compile a simple feedforward neural network.\"\"\"\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "# Print model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc15_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> b)  Entra√Ænement d‚Äôun mod√®le de classification sur MNIST</b></div>](#toc15_)\n",
    "\n",
    "**Entra√Ænement du mod√®le** :\n",
    "   - Utilisez la m√©thode `model.fit()` pour entra√Æner le mod√®le sur les donn√©es.\n",
    "   - Sp√©cifiez les param√®tres suivants :\n",
    "     - `ds_train` comme ensemble d‚Äôentra√Ænement.\n",
    "     - `ds_validation` comme ensemble de validation avec le param√®tre `validation_data`.\n",
    "     - `EPOCHS` comme nombre d‚Äô√©poques d‚Äôentra√Ænement.\n",
    "\n",
    "**Sauvegarde de l‚Äôhistorique** :\n",
    "   - Assurez-vous que l'objet `history` contient les informations de l‚Äôentra√Ænement pour pouvoir analyser les m√©triques d‚Äôentra√Ænement et de validation ult√©rieurement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualize training metrics\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation loss and accuracy.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.6)\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc16_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> c) Afficher les poids et la taille du mod√®le avant la quantification </b></div>](#toc16_)\n",
    "\n",
    "\n",
    "**Consigne :**  \n",
    "1. Affichez les poids des diff√©rentes couches de votre mod√®le √† l'aide de la m√©thode `get_weights()`.  \n",
    "2. Calculez la taille des poids pour chaque couche en m√©gaoctets (MB) et affichez ces informations de mani√®re structur√©e.  \n",
    "3. Calculez la taille totale du mod√®le sauvegard√© en utilisant `os.path.getsize()` pour √©valuer la m√©moire occup√©e avant la quantification.  \n",
    "\n",
    "  \n",
    "Cette √©tape vous permettra de comprendre la r√©partition de la m√©moire utilis√©e par un mod√®le non quantifi√© et de comparer ces r√©sultats avec ceux du mod√®le quantifi√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print weights\n",
    "print(\"\\nModel Weights:\")\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if weights:  # Skip layers with no weights (e.g., activations, flatten layers)\n",
    "        print(f\"Layer: {layer.name}\")\n",
    "        for i, weight in enumerate(weights):\n",
    "            print(f\"  Weight {i}: shape = {weight.shape}, size = {weight.nbytes / 1e3:.2f} KB\")\n",
    "            print(weight )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc17_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> d) Enregistrer le mod√®le dans votre espace de travail au format TensorFlow </b></div>](#toc17_)\n",
    "\n",
    "\n",
    "**Consigne :**  \n",
    "\n",
    "1. **Cr√©ation des r√©pertoires** :\n",
    "   - Cr√©ez un r√©pertoire pour stocker le mod√®le sauvegard√©.\n",
    "   - Assurez-vous que le r√©pertoire existe avant de tenter d'y √©crire.\n",
    "\n",
    "2. **Sauvegarde du mod√®le** :\n",
    "   - Sauvegardez le mod√®le dans le r√©pertoire sp√©cifi√© en utilisant `model.export()` ou une m√©thode similaire.\n",
    "\n",
    "3. **√âvaluation du mod√®le** :\n",
    "   - √âvaluez les performances du mod√®le sur l‚Äôensemble de test (`ds_test`) en utilisant `model.evaluate()`.\n",
    "   - Affichez les valeurs de la perte (`loss`) et de la pr√©cision (`accuracy`).\n",
    "\n",
    "4. **Calcul de la taille du mod√®le** :\n",
    "   - Calculez la taille totale du mod√®le sauvegard√© (en MB) en parcourant tous les fichiers du r√©pertoire de sauvegarde.\n",
    "   - Affichez la taille du mod√®le en utilisant un format lisible.\n",
    "\n",
    "\n",
    "Cette √©tape pr√©pare le mod√®le pour des √©tapes futures notamment la quantification.\n",
    "\n",
    "\n",
    "Cette √©tape pr√©pare le mod√®le pour des √©tapes futures notamment la quantification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc18_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> e) Fonctions Utilitaires </b></div>](#toc18_)\n",
    "\n",
    "#### **i. `print_stats_model`**\n",
    "\n",
    "**Objectif :**  \n",
    "Analyser les statistiques et les d√©tails des tenseurs d‚Äôun mod√®le TFLite quantifi√©.\n",
    "\n",
    "**Arguments :**  \n",
    "- `model` : Chemin du fichier TFLite.  \n",
    "- `m_name` : Nom du mod√®le pour l‚Äôaffichage.  \n",
    "- `show_weights` : Affiche les valeurs des poids si activ√©.\n",
    "\n",
    "**Exemple d‚Äôutilisation :**  \n",
    "```python\n",
    "print_stats_model(\"mnist_model_fp16.tflite\", m_name=\"MNIST Quantized Model\", show_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats_model(model, m_name=\"model\", show_weights=False):\n",
    "    \"\"\"\n",
    "    Prints statistics and details about a quantized TFLite model.\n",
    "\n",
    "    Args:\n",
    "        model (str): Path to the TFLite model file.\n",
    "        m_name (str): Model name for display.\n",
    "        show_weights (bool): Whether to display weight values. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Print model information\n",
    "        print(f\"\\nStats for: {m_name}\")\n",
    "        \n",
    "        # Evaluate model size\n",
    "        model_size = os.path.getsize(model)\n",
    "        print(f\"Model size: {model_size / 1e6:.2f} MB\")\n",
    "\n",
    "        # Load the TFLite model\n",
    "        interpreter = tf.lite.Interpreter(model_path=model)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # Analyze and print tensor details\n",
    "        tensor_details = interpreter.get_tensor_details()\n",
    "        total_weight_size = 0\n",
    "        print(\"\\nQuantized Model Details:\")\n",
    "        \n",
    "        for tensor in tensor_details:\n",
    "            print(f\"Tensor Name: {tensor['name']}\")\n",
    "            print(f\"  Shape: {tensor['shape']}\")\n",
    "            print(f\"  Data Type: {tensor['dtype']}\")\n",
    "            print(f\"  Quantization Parameters: {tensor['quantization']}\")\n",
    "            \n",
    "            # Get the actual tensor data\n",
    "            weights = interpreter.tensor(tensor['index'])()\n",
    "            weight_size = weights.nbytes / 1e3  # Convert to KB\n",
    "            total_weight_size += weight_size\n",
    "            print(f\"  Size: {weight_size:.2f} KB\")\n",
    "            \n",
    "            if show_weights:\n",
    "                print(f\"  Weights Values: {weights}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        # Print total weight size\n",
    "        print(f\"\\nTotal Weights Size: {total_weight_size / 1e3:.2f} MB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ii. `evaluate_quantized_model`**\n",
    "\n",
    "**Objectif :**  \n",
    "Tester les performances d‚Äôun mod√®le TFLite quantifi√© en √©valuant sa pr√©cision sur un dataset donn√©.\n",
    "\n",
    " **Arguments :**\n",
    "- `model_path` : Chemin vers le fichier TFLite contenant le mod√®le quantifi√©.\n",
    "- `dataset` : Dataset TensorFlow utilis√© pour l‚Äô√©valuation, comme `ds_test`.\n",
    "- `m_name` : Nom du mod√®le pour l‚Äôaffichage (par d√©faut : `\"Quantized Model\"`).\n",
    "\n",
    " **Fonctionnalit√©s :**\n",
    "1. Charge le mod√®le TFLite √† partir du chemin fourni.\n",
    "2. Affiche les d√©tails des entr√©es du mod√®le, comme la forme (`shape`) et le type (`dtype`).\n",
    "3. Boucle sur le dataset d‚Äô√©valuation :\n",
    "   - V√©rifie et ajuste la forme des entr√©es si n√©cessaire.\n",
    "   - Effectue des inf√©rences en utilisant le mod√®le TFLite.\n",
    "   - Compare les pr√©dictions aux labels r√©els pour calculer la pr√©cision.\n",
    "4. Affiche la pr√©cision finale (accuracy) du mod√®le.\n",
    "\n",
    " **Exemple d‚Äôutilisation :**\n",
    " \n",
    "```python\n",
    "evaluate_quantized_model(\"mnist_model_fp16.tflite\", ds_test, m_name=\"MNIST Float16 Quantized Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quantized_model(model_path, dataset, m_name=\"Quantized Model\"):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a quantized TFLite model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the TFLite model file.\n",
    "        dataset (tf.data.Dataset): A TensorFlow dataset for evaluation.\n",
    "        m_name (str): Name of the model for display.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Error: Model file '{model_path}' does not exist.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nEvaluating {m_name}\")\n",
    "\n",
    "        # Load the TFLite model\n",
    "        interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # Get input and output details\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "\n",
    "        # Check input shape and type\n",
    "        input_shape = input_details[0]['shape']\n",
    "        input_dtype = input_details[0]['dtype']\n",
    "\n",
    "        print(f\"Model Input Shape: {input_shape}\")\n",
    "        print(f\"Model Input Data Type: {input_dtype}\")\n",
    "\n",
    "        # Initialize counters for accuracy calculation\n",
    "        # Test the model\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for images, labels in ds_test:\n",
    "            # Remove the batch dimension for single image inference\n",
    "            input_data = tf.squeeze(images, axis=-1).numpy()  # From [1, 28, 28, 1] to [28, 28, 1]\n",
    "\n",
    "             # Ensure input_data matches the expected dtype\n",
    "            #input_data = input_data.astype(input_details[0]['dtype'])\n",
    "            # Adjust input data based on the model's expected dtype\n",
    "            if input_dtype == np.uint8:\n",
    "                input_data = (input_data * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "            elif input_dtype == np.float32:\n",
    "                input_data = input_data.astype(np.float32)  # Keep in [0, 1] range\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported input data type: {input_dtype}\")\n",
    "\n",
    "            # Set the tensor to the input data\n",
    "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "            # Run inference\n",
    "            interpreter.invoke()\n",
    "\n",
    "            # Get the prediction\n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            predicted_label = np.argmax(output_data)\n",
    "\n",
    "            # Compare with the true label\n",
    "            if predicted_label == labels.numpy():\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / total_predictions * 100\n",
    "        print(f\"Accuracy of the quantized {model_path}: {accuracy:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating the model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc20_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> f) Appliquer une r√©duction flottante : Quantification FP32 ‚Üí FP16 </b></div>](#toc20_)\n",
    "\n",
    "\n",
    "\n",
    " **Objectif :**\n",
    " Appliquer une quantification post-entra√Ænement au mod√®le sauvegard√© en convertissant celui-ci en format TFLite avec Float16 et en sauvegardant la version quantifi√©e. Cela permet d‚Äô√©conomiser de l‚Äôespace de stockage et d‚Äôacc√©l√©rer l‚Äôinf√©rence sur des appareils compatibles avec le calcul FP16.\n",
    "\n",
    " **√âtapes √† suivre :**\n",
    "\n",
    "\n",
    "1. **Chargement du mod√®le sauvegard√©** :\n",
    "   - Utilisez `TFLiteConverter.from_saved_model()` pour charger le mod√®le pr√©alablement sauvegard√© depuis le r√©pertoire sp√©cifi√©.\n",
    "\n",
    "2. **Application des optimisations** :\n",
    "\n",
    "3. **Configuration de la quantification Float16** :\n",
    "\n",
    "4. **Conversion du mod√®le** :\n",
    "\n",
    "5. **Sauvegarde du mod√®le quantifi√©** :\n",
    "   - Sauvegardez le mod√®le Float16 TFLite dans un fichier avec une extension `.tflite`.\n",
    "   - Assurez-vous que le r√©pertoire cible existe avant de sauvegarder le fichier.\n",
    "\n",
    "6. **Affichage de confirmation** :\n",
    "   - Affichez un message confirmant la r√©ussite de la conversion et la localisation du fichier quantifi√©.\n",
    "\n",
    "\n",
    " **Ressources compl√©mentaires :**\n",
    "\n",
    "Pour en savoir plus sur la quantification flottante avec TensorFlow Lite, consultez cet [article de blog](https://blog.tensorflow.org/2019/08/tensorflow-model-optimization-toolkit_5.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Afficher des statistiques sur le mod√®le quantifi√© en FP16**\n",
    "\n",
    " **Objectif :**\n",
    "Analyser le mod√®le quantifi√© en FP16 pour obtenir des informations d√©taill√©es sur sa structure, ses dimensions, les param√®tres de quantification et la taille de ses poids.\n",
    "\n",
    " **√âtapes √† suivre :** utiliser les foncttions utilitaires\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc21_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> e) Appliquer une Quantification FP32 --> INT8 et enregistrer le nouveau mod√®le quantifi√©</b></div>](#toc21_)\n",
    "\n",
    "\n",
    "**Objectif :**\n",
    "R√©duire la pr√©cision du mod√®le en passant de FP32 √† INT8 pour diminuer sa taille et am√©liorer ses performances sur des appareils aux ressources limit√©es.\n",
    "\n",
    " **√âtapes √† suivre :**\n",
    "\n",
    "1. **Configurer le convertisseur TFLite pour la quantification INT8 :**\n",
    "   - Charger le mod√®le au format TensorFlow.\n",
    "   - D√©finir une optimisation de type INT8.\n",
    "   - Sp√©cifier un jeu de donn√©es repr√©sentatif pour calibrer la quantification.\n",
    "\n",
    "2. **Effectuer la conversion :**\n",
    "   Convertir le mod√®le TensorFlow en un mod√®le TFLite quantifi√© en INT8.\n",
    "\n",
    "3. **Enregistrer le mod√®le converti :**\n",
    "   Sauvegarder le mod√®le quantifi√© au format `.tflite`.\n",
    "\n",
    "**Ressources compl√©mentaires :**\n",
    "\n",
    "Pour en savoir plus sur la quantification enti√®re avec TensorFlow Lite, consultez cet [article de blog](https://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html?hl=fr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Afficher des statistiques sur le mod√®le quantifi√© en INT8**\n",
    "\n",
    " **Objectif :**\n",
    "Analyser le mod√®le quantifi√© en INT8 pour obtenir des informations d√©taill√©es sur sa structure, ses dimensions, les param√®tres de quantification et la taille de ses poids.\n",
    "\n",
    " **√âtapes √† suivre :** utiliser les foncttions utilitaires\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc22_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 3.2 Quantification pendant l'entra√Ænement (Quantization Aware Training : QAT) </b></div>](#toc22_)\n",
    "\n",
    "\n",
    "# <a id='toc23_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> a) Quantification compl√®te du mod√®le </b></div>](#toc23_)\n",
    "\n",
    "\n",
    "\n",
    "**Objectif :**  \n",
    "Dans cette section, nous appliquons la quantification pendant l'entra√Ænement √† un mod√®le complet. Cela consiste √† int√©grer des op√©rations de quantification et de d√©quantification dans le flux d'entra√Ænement du mod√®le, afin d'am√©liorer ses performances en conditions r√©elles tout en r√©duisant sa taille.\n",
    "\n",
    "**Consigne :**  \n",
    "- Impl√©mentez la quantization aware training (QAT) sur l'int√©gralit√© du mod√®le. \n",
    "- Entra√Ænez le mod√®le sur le dataset MNIST pour d√©montrer l'efficacit√© de cette technique.\n",
    "- Comparez les performances du mod√®le quantifi√© avec celles du mod√®le original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc24_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> b) Quantification partielle du mod√®le </b></div>](#toc24_)\n",
    "\n",
    "**Objectif :**  \n",
    "Dans cette section, nous appliquons la quantization aware training (QAT) uniquement √† certaines couches du mod√®le. Cela permet de maintenir un compromis entre la r√©duction de la taille et la conservation de performances √©lev√©es.\n",
    "\n",
    "**Consigne :**  \n",
    "- S√©lectionnez uniquement la couche dense du mod√®le pour appliquer la quantification.\n",
    "- Entra√Ænez le mod√®le sur le dataset MNIST en int√©grant cette quantification partielle.\n",
    "- Comparez les performances du mod√®le quantifi√© partiellement avec celles du mod√®le original et du mod√®le enti√®rement quantifi√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc25_'></a>[<div style=\"text-align:left; border-radius:8px; padding:8px; color:white; margin:10px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#1E90FF;\"><b> üìä 4. √âvaluation et comparaison </b></div>](#toc25_)\n",
    "\n",
    "\n",
    "  ---\n",
    "  ### <a id='toc1_1_1_'></a>[**Table des mati√®res**](#toc0_)\n",
    "\n",
    "- [1. Introduction √† la quantification](#toc1_)    \n",
    "  - [1.1 Types de quantification](#toc3_)    \n",
    "  - [1.2 Les compromis √† prendre en compte](#toc4_)    \n",
    "- [üîç 2. √âchauffement : Bases de la quantification](#toc5_)    \n",
    "  - [2.1 Cr√©ation d'une s√©rie de valeurs](#toc6_)    \n",
    "  - [2.2 Quantification Asym√©trique](#toc7_)    \n",
    "  - [2.3 Quantification Sym√©trique](#toc8_)    \n",
    "  - [2.4 M√©thode de quantification par Percentile](#toc9_)    \n",
    "  - [2.5 Comparaison : Symetrique, Asymetrique, Min-Max , Percentile](#toc10_)    \n",
    "  - [2.6 Analyse de l'impact du nombre des bits sur l'erreur](#toc11_)    \n",
    "- [üñºÔ∏è 3. Application aux Mod√®les Complexes](#toc12_)    \n",
    "  - [3.1 Quantification apr√©s entrainement (Post Training Quantization PTQ)](#toc13_)    \n",
    "  - [3.2 Quantification pendant l'entra√Ænement (Quantization Aware Training)](#toc22_)    \n",
    " \n",
    "- [üìä 4. √âvaluation et comparaison](#toc25_)    \n",
    "  - [4.1 Comparaison des tailles et des performances des mod√®les](#toc26_)    \n",
    "  - [4.2 Conclusion](#toc29_)      \n",
    "- [Bonus](#toc32_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "    numbering=false\n",
    "    anchor=true\n",
    "    flat=false\n",
    "    minLevel=1\n",
    "    maxLevel=6\n",
    "    /vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n",
    "  ---\n",
    "  \n",
    "# <a id='toc26_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 4.1 Comparaison des tailles et des performances des mod√®les </b></div>](#toc26_)\n",
    "\n",
    "\n",
    "**Objectif :**  \n",
    "L'objectif de cette section est d'analyser l'impact de la quantification sur la taille et les performances des mod√®les. Nous allons comparer les mod√®les Float32 (mod√®le original), Float16 et INT8 en termes de taille et de pr√©cision.\n",
    "\n",
    "---\n",
    "# <a id='toc27_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> a) √âtape 1 : Afficher la taille des mod√®les </b></div>](#toc27_)\n",
    "\n",
    "\n",
    "Utilisez la fonction `print_stats_model` pour calculer et afficher les tailles respectives des mod√®les Float32, Float16 et INT8. Cela permettra d'observer la r√©duction de taille obtenue gr√¢ce √† chaque m√©thode de quantification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc28_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> b) √âtape 2 : Comparer les performances des mod√®les </b></div>](#toc28_)\n",
    "\n",
    "#### **Objectif :**\n",
    "√âvaluer et comparer les performances des mod√®les Float32, Float16 et INT8 sur le jeu de test pour analyser l'impact de la quantification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc29_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:70%; font-family:Arial, sans-serif; background-color:#f17c12;\"><b> 4.2 Conclusion </b></div>](#toc29_)\n",
    "\n",
    "# <a id='toc30_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> a)  Flottant ou enti√®re </b></div>](#toc30_)\n",
    "\n",
    "1. **Taille des mod√®les :**  \n",
    "   - La quantification Float16 permet une r√©duction significative de la taille du mod√®le tout en conservant une pr√©cision proche de celle du mod√®le original Float32.\n",
    "   - La quantification INT8 offre une r√©duction de taille encore plus importante, au prix d'une √©ventuelle perte de pr√©cision.\n",
    "\n",
    "2. **Performances des mod√®les :**  \n",
    "   - Le mod√®le Float16 conserve une pr√©cision √©lev√©e, rendant cette m√©thode id√©ale lorsque les contraintes de m√©moire ne sont pas strictes.  \n",
    "   - Le mod√®le INT8 peut subir une d√©gradation plus marqu√©e de la pr√©cision, mais reste adapt√© pour des environnements o√π les contraintes de stockage et de calcul sont prioritaires (comme les appareils embarqu√©s).\n",
    "\n",
    "3. **Compromis taille-pr√©cision :**  \n",
    "   - Le choix de la m√©thode de quantification d√©pend des besoins sp√©cifiques de votre application :  \n",
    "     - **Float16 :** Meilleur compromis entre taille et pr√©cision.  \n",
    "     - **INT8 :** R√©duction maximale de la taille pour des applications n√©cessitant une faible empreinte m√©moire.\n",
    "\n",
    "\n",
    "La quantification constitue une technique puissante pour optimiser les mod√®les tout en r√©pondant aux contraintes mat√©rielles sp√©cifiques. Il est essentiel d‚Äô√©valuer chaque approche dans le contexte de votre cas d‚Äôutilisation pour choisir la m√©thode optimale.\n",
    "\n",
    "# <a id='toc31_'></a>[<div style=\"text-align:left; border-radius:6px; padding:6px; color:white; margin:5px 0; font-size:50%; font-family:Arial, sans-serif; background-color:#115175;\"><b> b) Int√©grale ou partielle </b></div>](#toc31_)\n",
    "\n",
    "\n",
    "1. **Quantification int√©grale :**  \n",
    "   - La quantification compl√®te d'un mod√®le (toutes les couches) permet une r√©duction maximale de la taille et des ressources n√©cessaires pour l'inf√©rence.  \n",
    "   - Cependant, elle peut entra√Æner une d√©gradation de pr√©cision plus importante, en particulier pour les mod√®les complexes ou les t√¢ches n√©cessitant une grande sensibilit√©.\n",
    "\n",
    "2. **Quantification partielle :**  \n",
    "   - La quantification partielle (appliqu√©e uniquement √† certaines couches, par exemple les couches enti√®rement connect√©es ou convolutives) offre un compromis int√©ressant.  \n",
    "   - Elle permet de r√©duire la taille et les besoins en calcul tout en limitant l‚Äôimpact sur la pr√©cision globale du mod√®le.  \n",
    "   - Cette approche est particuli√®rement utile pour des mod√®les critiques, o√π certaines couches doivent conserver leur pr√©cision d'origine pour garantir des performances optimales.\n",
    "\n",
    "3. **Choix entre int√©grale et partielle :**  \n",
    "   - Le choix entre une quantification int√©grale ou partielle d√©pend des priorit√©s de votre application :  \n",
    "     - **Quantification int√©grale :** Convient aux environnements contraints en m√©moire et en calcul, o√π une certaine d√©gradation de pr√©cision est acceptable.  \n",
    "     - **Quantification partielle :** Convient lorsque la pr√©cision est une priorit√© tout en b√©n√©ficiant d'une optimisation partielle des ressources.\n",
    "\n",
    "La quantification partielle est souvent privil√©gi√©e pour son √©quilibre entre r√©duction des ressources et pr√©servation des performances. Une analyse approfondie du mod√®le et des besoins sp√©cifiques de l'application est n√©cessaire pour faire le bon choix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc32_'></a>[<div style=\"text-align:center; border-radius:20px; padding:18px; color:white; margin:10px 0; font-size:100%; font-family:Arial, sans-serif; background-color:#1E90FF;\"><b>üéÅ Bonus</b></div>](#toc0_)\n",
    "<div style=\"text-align: center; margin: 40px 0;\">\n",
    "    <a href=\"https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Boundless_TFLite.ipynb\" target=\"_parent\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"width: 350px; height: auto;\">\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "  ---\n",
    "  ### <a id='toc1_1_1_'></a>[**Table des mati√®res**](#toc0_)\n",
    "\n",
    "- [1. Introduction √† la quantification](#toc1_)    \n",
    "  - [1.1 Types de quantification](#toc3_)    \n",
    "  - [1.2 Les compromis √† prendre en compte](#toc4_)    \n",
    "- [üîç 2. √âchauffement : Bases de la quantification](#toc5_)    \n",
    "  - [2.1 Cr√©ation d'une s√©rie de valeurs](#toc6_)    \n",
    "  - [2.2 Quantification Asym√©trique](#toc7_)    \n",
    "  - [2.3 Quantification Sym√©trique](#toc8_)    \n",
    "  - [2.4 M√©thode de quantification par Percentile](#toc9_)    \n",
    "  - [2.5 Comparaison : Symetrique, Asymetrique, Min-Max , Percentile](#toc10_)    \n",
    "  - [2.6 Analyse de l'impact du nombre des bits sur l'erreur](#toc11_)    \n",
    "- [üñºÔ∏è 3. Application aux Mod√®les Complexes](#toc12_)    \n",
    "  - [3.1 Quantification apr√©s entrainement (Post Training Quantization PTQ)](#toc13_)    \n",
    "  - [3.2 Quantification pendant l'entra√Ænement (Quantization Aware Training)](#toc22_)    \n",
    " \n",
    "- [üìä 4. √âvaluation et comparaison](#toc25_)    \n",
    "  - [4.1 Comparaison des tailles et des performances des mod√®les](#toc26_)    \n",
    "  - [4.2 Conclusion](#toc29_)      \n",
    "- [Bonus](#toc32_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "    numbering=false\n",
    "    anchor=true\n",
    "    flat=false\n",
    "    minLevel=1\n",
    "    maxLevel=6\n",
    "    /vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n",
    "  ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
